{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207aa80-9f91-41c8-97b3-c65d2ffcbdd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install accelerate>=0.26.0\n",
    "!pip install datasets\n",
    "!pip install -U bitsandbytes\n",
    "!pip install sae_lens\n",
    "!pip install huggingface_hub\n",
    "import json\n",
    "import os\n",
    "import datasets\n",
    "from sae_lens import SAE\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb053710-4eb2-4602-b5ef-8f6ac989518f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "model = HookedTransformer.from_pretrained(model_name, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2962eaf-0664-4a47-ac9e-6fc8179f0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(release=\"llama_scope_lxr_32x\", sae_id=\"l25r_32x\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc26f11-337b-4e8a-8120-504ce6187b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hook point\n",
    "hook_point = sae.cfg.hook_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5f65b-efcc-4283-88e4-5b059638efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5cf6b8-a4b4-49b9-b68e-a993853bb041",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions = [item[\"question\"] for item in dataset[\"train\"]]\n",
    "test_questions = [item[\"question\"] for item in dataset[\"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e4785-9638-43cc-873a-57de3afc641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from sae_lens import SAE\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "N = 800\n",
    "\n",
    "role_play_prompt_set = [\n",
    "    # English (英语)\n",
    "    \"As a highly qualified mathematics teacher, you excel at solving problems systematically and explaining solutions with clarity. I am your student, eager to learn. Please solve the following problem:\",\n",
    "\n",
    "    # Chinese (中文)\n",
    "    \"从现在开始，你是一位优秀的数学老师，总是正确地教授学生数学问题。我是你的学生之一，请回答以下问题。\",\n",
    "\n",
    "    # German (德语)\n",
    "    \"Sie sind ein angesehener Mathematikprofessor mit umfassender Erfahrung in der präzisen Lösung komplexer Probleme. Ich bin Ihr Schüler und kenne die Antwort auf die folgende Frage nicht. Können Sie mir helfen? Die Frage ist:\",\n",
    "\n",
    "    # French (法语)\n",
    "    \"Vous êtes un professeur de mathématiques de renommée mondiale, expert dans la résolution précise et efficace des problèmes. En tant que votre étudiant, je suis bloqué sur une question depuis longtemps. Pourriez-vous résoudre la tâche suivante pour moi?\",\n",
    "\n",
    "    # Japanese (日语)\n",
    "    \"あなたは数学の専門家であり、深い問題解決のスキルを持っています。私はあなたの生徒で、宿題に困っています。先生として、次の問題を解いてください。\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "random.shuffle(role_play_prompt_set)  \n",
    "role_play_cycle = cycle(role_play_prompt_set)  \n",
    "\n",
    "selected_questions = [random.choice(train_questions) for _ in range(N)]\n",
    "\n",
    "\n",
    "with_role_prompts = [\n",
    "    f\"{next(role_play_cycle)} {q}\"\n",
    "    for q in selected_questions\n",
    "]\n",
    "without_role_prompts = [\n",
    "    f\"{q}\"\n",
    "    for q in selected_questions\n",
    "]\n",
    "\n",
    "# with_role_prompts = [\n",
    "#     f\"{next(role_play_cycle)} {q} Please output your final answer (a number) after 'Output: '\"\n",
    "#     for q in selected_questions\n",
    "# ]\n",
    "# without_role_prompts = [\n",
    "#     f\"{q} Please output your final answer (a number) after 'Output: '\"\n",
    "#     for q in selected_questions\n",
    "# ]\n",
    "\n",
    "with_role_feature_acts_list = []\n",
    "without_role_feature_acts_list = []\n",
    "\n",
    "for i in tqdm(range(N), desc=\"Processing Prompts\", unit=\"pair\"):\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "       \n",
    "        with_role_prompt = with_role_prompts[i]\n",
    "        without_role_prompt = without_role_prompts[i]\n",
    "\n",
    "        with_role_logits, with_role_cache = model.run_with_cache(with_role_prompt, prepend_bos=True)\n",
    "        without_role_logits, without_role_cache = model.run_with_cache(without_role_prompt, prepend_bos=True)\n",
    "\n",
    "        with_role_feature_acts = sae.encode(with_role_cache[hook_point])\n",
    "        without_role_feature_acts = sae.encode(without_role_cache[hook_point])\n",
    "\n",
    "        with_role_last_token_index = with_role_feature_acts.shape[1] - 1\n",
    "        without_role_last_token_index = without_role_feature_acts.shape[1] - 1\n",
    "\n",
    "        with_role_last_token_activations = with_role_feature_acts[0, with_role_last_token_index, :]\n",
    "        without_role_last_token_activations = without_role_feature_acts[0, without_role_last_token_index, :]\n",
    "\n",
    "        with_role_feature_acts_list.append(with_role_last_token_activations)\n",
    "        without_role_feature_acts_list.append(without_role_last_token_activations)\n",
    "\n",
    "    \n",
    "    del with_role_cache, without_role_cache, with_role_feature_acts, without_role_feature_acts\n",
    "    del with_role_logits, without_role_logits\n",
    "    torch.cuda.empty_cache()  \n",
    "    torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28ecdd-4684-482c-95ad-2b0ecc4961c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with_role_feature_acts = torch.stack(with_role_feature_acts_list)  # (N, feature_dim)\n",
    "without_role_feature_acts = torch.stack(without_role_feature_acts_list)  # (N, feature_dim)\n",
    "\n",
    "delta_h = ((with_role_feature_acts > 0).float() - (without_role_feature_acts > 0).float())\n",
    "\n",
    "sensitivity_score = (delta_h > 0).float().mean(dim=0)  \n",
    "k = 15\n",
    "top_k_indices = torch.topk(sensitivity_score, k=k).indices\n",
    "top_k_features = sensitivity_score[top_k_indices]\n",
    "\n",
    "\n",
    "print(f\"Top-{k} Role-Play Influenced Features (Indices & Sensitivity Scores):\")\n",
    "for idx, score in zip(top_k_indices.tolist(), top_k_features.tolist()):\n",
    "    print(f\"Feature {idx}: Sensitivity {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_activations = with_role_feature_acts[:, top_k_indices]  # (N, k)\n",
    "\n",
    "mean_activation = top_k_activations.mean(dim=0)  # (k,)\n",
    "\n",
    "std_activation = top_k_activations.std(dim=0)  # (k,)\n",
    "\n",
    "β = -10\n",
    "\n",
    "steering_strengths = mean_activation + β * std_activation  # (k,)\n",
    "\n",
    "steering_vectors = sae.W_dec[top_k_indices]  \n",
    "\n",
    "steering_shift = (steering_strengths[:, None] * steering_vectors).sum(dim=0)  # (feature_dim,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd344f3f-72aa-4197-b7a8-f4b97ba91793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_indices = torch.tensor([72224, 7863, 78558, 128199, 63388, 15987, 26076], device=with_role_feature_acts.device)\n",
    "\n",
    "# selected_activations = with_role_feature_acts[:, selected_indices]  # (N, selected_k)\n",
    "\n",
    "# mean_activation = selected_activations.mean(dim=0)  # (selected_k,)\n",
    "# std_activation = selected_activations.std(dim=0)  # (selected_k,)\n",
    "\n",
    "# β = 1\n",
    "\n",
    "# steering_strengths = mean_activation + β * std_activation  # (selected_k,)\n",
    "\n",
    "# steering_vectors = sae.W_dec[selected_indices]  # (selected_k, feature_dim)\n",
    "\n",
    "# steering_shift = (steering_strengths[:, None] * steering_vectors).sum(dim=0)  # (feature_dim,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae72e6e-c081-485e-930e-ff6169d2eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering_hook(resid_pre, hook):\n",
    "    \n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return\n",
    "\n",
    "    position = resid_pre.shape[1] - 1 \n",
    "    \n",
    "    if steering_on:\n",
    "        \n",
    "        orig_norm = torch.norm(resid_pre[:, position, :], p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        resid_pre[:, position, :] += steering_shift  \n",
    "\n",
    "        new_norm = torch.norm(resid_pre[:, position, :], p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        resid_pre[:, position, :] *= (orig_norm / new_norm)  \n",
    "\n",
    "\n",
    "def hooked_generate(prompt_batch, fwd_hooks=[], seed=None):\n",
    "    \n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        # result = model.generate(\n",
    "        #          input=tokenized,\n",
    "        #          max_new_tokens=150,       \n",
    "        #           )\n",
    "        \n",
    "        result = model.generate(\n",
    "                 input=tokenized,\n",
    "                 max_new_tokens=150,\n",
    "                 do_sample=True,   \n",
    "                 temperature=0.7,  \n",
    "                 top_k=50,         \n",
    "                 top_p=0.9,        \n",
    "                  )\n",
    "    \n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2319ed3-9cf1-4ea3-b574-b90fd257951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generate(example_prompt):\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    editing_hooks = [(\"blocks.25.hook_resid_post\", steering_hook)]\n",
    "    res = hooked_generate(\n",
    "        [example_prompt], editing_hooks, seed=None\n",
    "    )\n",
    "\n",
    "    res_str = model.to_string(res[:, 1:])  \n",
    "    return res_str[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09d828-0d64-4246-abf6-f73e019e0164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Function to extract 8-shot examples from training data\n",
    "def extract_8_shot_examples(dataset, num_shots=8):\n",
    "    \n",
    "    examples = []\n",
    "    for item in dataset.select(range(num_shots)):  # Select first 'num_shots' examples\n",
    "        question = item[\"question\"]\n",
    "        answer_text = item[\"answer\"]\n",
    "\n",
    "        # Extract final numeric answer using regex (#### number)\n",
    "        match = re.search(r\"####\\s*([\\d\\.\\-]+)\", answer_text)\n",
    "        final_answer = match.group(1) if match else \"N/A\"\n",
    "\n",
    "        # Format example with CoT reasoning\n",
    "        formatted_example = f\"Q: {question}\\nA: Let's think step by step.\\n{answer_text.strip()}\\nOutput: {final_answer}\"\n",
    "        examples.append(formatted_example)\n",
    "    return examples\n",
    "\n",
    "\n",
    "eight_shot_examples = extract_8_shot_examples(dataset[\"train\"])\n",
    "\n",
    "def generate_prompt(question):\n",
    "    prompt = \"\\n\".join(eight_shot_examples)  # Add 8-shot examples\n",
    "    prompt += f\"\\nQ: {question}\\nA: Let's think step by step.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf433a-8cfe-43e3-9ab8-1bce7edef55d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steering_on = True\n",
    "\n",
    "\n",
    "# Track accuracy\n",
    "correct = 0\n",
    "valid_instances = 0\n",
    "\n",
    "\n",
    "# Iterate through test questions\n",
    "for i in tqdm(range(len(test_questions)), desc=\"Generating Responses\", unit=\"prompt\"):\n",
    "    question = test_questions[i]\n",
    "    example_prompt = generate_prompt(question)  # Add 8-shot context\n",
    "\n",
    "    # Generate response\n",
    "    generated_text = run_generate(example_prompt)  \n",
    "\n",
    "    # Extract only model's response\n",
    "    prompt_len = len(example_prompt)  # Get length of input prompt\n",
    "    model_response = generated_text[prompt_len:].strip()  # Extract model's generated answer\n",
    "\n",
    "    # Extract numeric answer from model output (after 'Output:')\n",
    "    match = re.search(r\"Output:\\s*\\$?([\\d,\\.]+)\", model_response)\n",
    "    \n",
    "    if match:\n",
    "            model_answer = match.group(1).replace(\",\", \"\").rstrip(\".\")\n",
    "            if \".\" in model_answer and float(model_answer).is_integer():\n",
    "                model_answer = str(int(float(model_answer)))\n",
    "                \n",
    "    if not match:\n",
    "        continue  \n",
    "\n",
    "        \n",
    "    # Extract correct answer from dataset\n",
    "    true_answer_match = re.search(r\"####\\s*([\\d\\.\\-]+)\", dataset[\"test\"][i][\"answer\"])\n",
    "    true_answer = true_answer_match.group(1) if true_answer_match else \"N/A\"\n",
    "\n",
    "    # Compare answers (numerical comparison)\n",
    "    if model_answer == true_answer:\n",
    "        correct += 1\n",
    "    valid_instances += 1  # Count only valid instances\n",
    "\n",
    "    # Print question and response\n",
    "    print(f\"Question {i+1}: {question}\")  \n",
    "    print(f\"Response {i+1}: {model_response}\")  \n",
    "    print(f\"Model Answer: {model_answer} | True Answer: {true_answer}\\n\")\n",
    "\n",
    "    # Free GPU memory\n",
    "    del generated_text\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# Compute accuracy (only on valid instances)\n",
    "accuracy = (correct / valid_instances * 100) if valid_instances > 0 else 0\n",
    "print(f\"\\nModel Accuracy (on valid instances only): {accuracy:.2f}%\")\n",
    "print(f\"Valid Instances: {valid_instances} / Total Test Cases: {len(dataset['test'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee86ed-6226-491d-bbfb-d79fa2005028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steering_on = False\n",
    "\n",
    "\n",
    "# Track accuracy\n",
    "correct = 0\n",
    "valid_instances = 0\n",
    "\n",
    "\n",
    "# Iterate through test questions\n",
    "for i in tqdm(range(len(test_questions)), desc=\"Generating Responses\", unit=\"prompt\"):\n",
    "    question = test_questions[i]\n",
    "    example_prompt = generate_prompt(question)  # Add 8-shot context\n",
    "\n",
    "    # Generate response\n",
    "    generated_text = run_generate(example_prompt)  \n",
    "\n",
    "    # Extract only model's response\n",
    "    prompt_len = len(example_prompt)  # Get length of input prompt\n",
    "    model_response = generated_text[prompt_len:].strip()  # Extract model's generated answer\n",
    "\n",
    "    # Extract numeric answer from model output (after 'Output:')\n",
    "    match = re.search(r\"Output:\\s*\\$?([\\d,\\.]+)\", model_response)\n",
    "    \n",
    "    if match:\n",
    "            model_answer = match.group(1).replace(\",\", \"\").rstrip(\".\")\n",
    "            if \".\" in model_answer and float(model_answer).is_integer():\n",
    "                model_answer = str(int(float(model_answer)))\n",
    "                \n",
    "    if not match:\n",
    "        continue  \n",
    "\n",
    "        \n",
    "    # Extract correct answer from dataset\n",
    "    true_answer_match = re.search(r\"####\\s*([\\d\\.\\-]+)\", dataset[\"test\"][i][\"answer\"])\n",
    "    true_answer = true_answer_match.group(1) if true_answer_match else \"N/A\"\n",
    "\n",
    "    # Compare answers (numerical comparison)\n",
    "    if model_answer == true_answer:\n",
    "        correct += 1\n",
    "    valid_instances += 1  # Count only valid instances\n",
    "\n",
    "    # Print question and response\n",
    "    print(f\"Question {i+1}: {question}\")  \n",
    "    print(f\"Response {i+1}: {model_response}\")  \n",
    "    print(f\"Model Answer: {model_answer} | True Answer: {true_answer}\\n\")\n",
    "\n",
    "    # Free GPU memory\n",
    "    del generated_text\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# Compute accuracy (only on valid instances)\n",
    "accuracy = (correct / valid_instances * 100) if valid_instances > 0 else 0\n",
    "print(f\"\\nModel Accuracy (on valid instances only): {accuracy:.2f}%\")\n",
    "print(f\"Valid Instances: {valid_instances} / Total Test Cases: {len(dataset['test'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04cc0c-950b-4f36-812c-61ac6f993c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
