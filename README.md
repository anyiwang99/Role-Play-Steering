# Role-Playing Steering with Sparse Autoencoders (SAE)

This project explores **role-specific behavior steering** in Large Language Models (LLMs) using features extracted by a **Sparse Autoencoder (SAE)**. By leveraging contrastive activation patterns between role-play and non-role-play inputs, our method systematically improves reasoning performance in both commonsense and arithmetic domains, while offering strong interpretability.

---

## ğŸ” Method Overview

We construct **contrastive input pairs**, where each pair includes:

- One sample with a **role-playing prompt**
- One sample without a role-playing prompt

By comparing their activation differences, we use a pretrained **Sparse Autoencoder (SAE)** to extract the **top-k most discriminative features**. These features are then **injected into the residual stream** of the model, guiding it toward role-specific behavior and improving reasoning ability.

---

## ğŸ§ª Experimental Setup

We evaluate our method across **three models**, **three datasets**, and **three evaluation settings**:

### ğŸ§  Models
- `Llama3.1-8B`
- `Gemma2-2B`
- `Gemma2-9B`

### ğŸ“š Datasets
- **GSM8K** â€“ arithmetic reasoning
- **SVAMP** â€“ arithmetic reasoning
- **CSQA** â€“ commonsense reasoning

### ğŸ§¾ Evaluation Settings
- **Zero-shot-CoT**
- **One-shot-CoT**
- **Few-shot-CoT**

---

## ğŸ“ˆ Results

Extensive experiments show that our **SAE-based steering method**:
- **Consistently improves performance** across models and tasks
- **Matches or even outperforms prompt-based role-playing**
- Offers **greater stability and interpretability** by exposing and controlling internal feature activations

---


## ğŸ“ Code Structure

``` 
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ train.csv # SVAMP training data (for arithmetic tasks)
â”‚ â”œâ”€â”€ SVAMP.json # SVAMP test data (arithmetic)
â”‚ â”œâ”€â”€ role_prompts_arithmetic.txt # Role-play prompts for arithmetic reasoning
â”‚ â”œâ”€â”€ role_prompts_commonsense.txt # Role-play prompts for commonsense reasoning
â”‚ â”œâ”€â”€ few_shot_prompts_arithmetic.json # Few-shot exemplars for arithmetic tasks
â”‚ â””â”€â”€ few_shot_prompts_commonsense.json # Few-shot exemplars for commonsense tasks
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ sae_pipeline.py # Main script to run evaluation with and without steering
``` 
---

## â–¶ï¸ Getting Started

### 1. Install dependencies
```bash
pip install -r requirements.txt
```
### 1. Run evaluation

```bash
python sae_pipeline.py
```
You can configure model name, dataset, evaluation setting, sae layer and other hyperparameters inside the script or via command-line arguments.
